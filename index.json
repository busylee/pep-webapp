[
{
	"uri": "/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": "Scalable Image Upload Web Application  이 실습에서는 다양한 AWS 빌딩 블록 (EC2, ELB, RDS, ElastiCache, S3, IAM, CloudWatch, AutoScaling 등)을 사용하여 확장 가능한 웹 애플리케이션을 구축하기 위한 내용을 구성하고 있습니다.\n주요 서비스 구성요소  EC2 ELB AutoScaling RDS Elasticache S3 IAM  데모 웹페이지 최종 서비스 구성도  "
},
{
	"uri": "/lab1/",
	"title": "실습1. Basic Version",
	"tags": [],
	"description": "",
	"content": "Level 1 – Basic Version (All in one)  Single Server Implementation Web Frontend (Apache + PHP) DB Backend (MYSQL) Upload to local storage  인스턴스 시작  이 랩에서는 단일 서버 위에 웹 애플리케이션 구동에 필요한 웹서버프로그램, 데이터베이스 엔진 그리고 프로그래밍에 필요한 프레임워크를 설치합니다. 그리고 개발한 소프트웨어를 배포하여 웹 애플리케이션의 동작을 확인합니다. 클라우드 환경에서 단일 서버는 EC2 이고 웹애플리케이션 구동에 필요한 소프트웨어는 Apache, PHP, MySQL 입니다. EC2 인스턴스로 사용할 이미지(AMI)는 Ubuntu 18.04 AMI 입니다.   EC2 생성에 필요한 단계를 진행하시고 웹서비스와 SSH 접속을 위해 80번과 22번 포트를 열어줍니다. EC2 생성이 완료된 후에는 SSH 로 접속하여 MySQL, PHP, Apache 소프트웨어를 설치하고 git 명령을 통해 웹애플리케이션 데모를 위한 개발코드를 다운로드 합니다.  소프트웨어 설치 MySQL 데이터베이스 설치\nsudo apt-get update\nsudo apt-get install mysql-server\nsudo mysql_secure_installation\n👉 type \u0026lsquo;Y\u0026rsquo; for followed question\nApache 와 PHP 설치\nsudo apt-get install apache2 php libapache2-mod-php php-mysql php-curl php-xml php-memcached awscli Apache 서버 재시작\nsudo service apache2 restart git 설치 및 데모용 웹애플리케이션 다운로드\nsudo apt-get install git cd /var sudo chown -R ubuntu:ubuntu www cd /var/www/html git clone https://github.com/qyjohn/web-demo cd web-demo 웹서비스용 디렉토리 권한 변경\n sudo chown -R www-data:www-data uploads* 데모용 웹애플리케이션 데이터베이스 생성\nsudo mysql mysql\\\u0026gt; CREATE DATABASE web_demo; mysql\\\u0026gt; CREATE USER \\'username\\'@\\'localhost\\' IDENTIFIED BY 'password\\'; mysql\\\u0026gt; GRANT ALL PRIVILEGES ON web_demo.\\* TO 'username\\'@\\'localhost\\'; mysql\\\u0026gt; quit 데모용 데이터 추가\n 개발소스코드의 Database 접속을 위한 데이터베이스이름, 사용자 이름과 비밀번호는 각각“web_demo”, “username” 그리고 “password” 입니다. * git 명령으로 다운로드 받은 소스코드 내에는 미리 입력된 web_demo.sql 라는 SQL 문이 포함되어 있으니 이를 import 하여Database에 데이터를 추가합니다. git 명령으로 다운로드 받은 소스코드 내에는 미리 입력된 web_demo.sql 라는 SQL 문이 포함되어 있으니 이를 import 하여Database에 데이터를 추가합니다.  cd /var/www/html/web-demo mysql -u username -p web_demo \u0026lt; web_demo.sql 주의사항\n 만약 데이터베이스이름, 사용자이름, 비밀번호를 변경하고 싶을 경우 vi 에디터를 이용하여config.php 의 내용을 직접 수정하시기 바랍니다. PC 에 있는 아무 인터넷브라우저 주소창에 http://ip-address/web-demo/index.php 를 입력하여 데모 애플리케이션을 확인해 보시기 바랍니다. 기타 데모 애플리케이션과 관련된 제약 사항은 아래 내용을 참고해주세요.   You can login with your name, then upload some photos for testing. (You might have noticed that this demo application does not ask you for a password. This is because we would like to make things as simple as possible. Handling user password is a very complicate issue, which is beyond the scope of this entry level tutorial.) Then I suggest that you spend 10 minutes reading through the demo code index.php. The demo code has reasonable documentation in the form of comments, so I am not going to explain the code here.\n  Now you are able to get your website working, please upload some more pictures for testing. Upload some small pictures and some big pictures (like 20 MB) to see what happens. Fix any issues you may observe in the tests.\n "
},
{
	"uri": "/lab2/",
	"title": "실습2. Load Balanced Solution",
	"tags": [],
	"description": "",
	"content": "Level 2 – Scale the application to two or more servers  Session sharing issue (ELB or Cache ?) Shared database issue (RDS) Shared Storage issue (EFS) NFS? Single point of failure  이번 레벨에서는 실습1 의 기본버전을 확장하여 여러 서버에 배포하고자 합니다. 파일 공유를 위해 NFS를 사용할 수도 있지만 (Amazon) EFS를 사용하면 쉽게 파일 공유 서비스를 사용할 수 있습니다.\nSTEP 1 - EFS File System 준비   Go to the EFS Console and create an EFS file system. (Note the DNS Name for EFS file system. You need this value for the next step)\n  Terminate the previous EC2 instance because we no longer need it. Launch a new EC2 instance with the Ubuntu 18.04 operating system. SSH into the EC2 instance to install the following software and mount the EFS file system:\n sudo apt-get update sudo apt-get install nfs-common sudo mkdir /efs sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs sudo chown -R ubuntu:ubuntu /efs    Then we add the mounting stuff into /etc/fstab to add the following line, so that you do not need to manually mount the EFS file system when the operating system is rebooted.\n\u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs nfs auto,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0   You can verify the above-mentioned configuration is working using the following commands (run them several times):\n$ df -h $ mount $ sudo umount /efs $ sudo mount /efs   STEP 2 - Install the Apache and PHP   Run the following commands to install Apache and PHP. Notice that we are not installing the MySQL server this time.\n sudo apt-get install apache2 php mysql-client libapache2-mod-php php-mysql php-curl php-xml awscli sudo service apache2 start    Then we use the EFS file system to store our web application.he EFS file system:\n$ cd /efs $ git clone https://github.com/qyjohn/web-demo $ cd web-demo $ sudo chown -R www-data:www-data uploads $ cd /var/www/html $ sudo ln -s /efs/web-demo web-demo   STEP 3 - Launch an RDS Instance  Launch an RDS instance running MySQL. When launching the RDS instance (Dev/Test-Mysql, t2.micro), create a default database named “web_demo”. When the RDS instance becomes available. Please make sure that the security group being used on the RDS instance allows inbound connection from your EC2 instance. Then, connect to the RDS instance and create a user for your application. This time, when granting privileges, you need to grant external access for the user.  mysql -h [endpoint-of-rds-instance] -u \u0026lt;master username\u0026gt; -p mysql\u0026gt; CREATE DATABASE web_demo;* mysql\u0026gt; CREATE USER \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON web_demo.* TO \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39;; mysql\u0026gt; quit  Then, use the following command to import the demo data in web_demo.sql to the web_demo database on the RDS database:  $ cd /var/www/html/web-demo $ mysql -h [endpoint-of-rds-instance] -u username -p web_demo \u0026lt; web_demo.sql  Now, modify config.php with the new database server hostname, username, password, and database name.  STEP 4 - Create an ElastiCacheMemcached Cluster   We use ElastiCache to resolve the session sharing issue between multiple web servers. In the ElastiCache console, launch an ElastiCache cluster with Memcached (just 1 single node is enough) and obtain the endpoint information. Please make sure that the security group being used on the ElastiCache cluster allows inbound connection from your EC2 instance.\n  On the web server, configure php.ini to use Memcached for session sharing.\n  Edit /etc/php/7.2/apache2/php.ini. Make the following modifications:\nsession.save_handler = memcached session.save_path = \u0026#34;[dns-endpoint-to-the-elasticache-node]:11211\u0026#34;   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcached $ sudo service apache2 restart   Edit /etc/php/7.2/mods-available/memcached.ini, add the following two lines to support session redundancy. Please note that the value of memcache.session_redundancy equals to the number of cache nodes plus 1 (because of a bug in PHP).\nmemcache.allow_failover=1 memcache.session_redundancy=2   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcache $ sudo service apache2 restart   STEP 5 - Create an AMI Now, create an AMI from the EC2 instance and launch a new EC2 instance with the AMI.\nSTEP 6 - Create an ELB Create an Application Load Balancer (ALB) and register the two EC2 instances to the ALB target group. Since we do have Apache running on both web servers, you might want to use HTTP as the ping protocol with 80 as the ping port and “/” as the ping path for the health check parameter for your ELB.\n Create Target Group and Register 2 EC2 instances that you created before Create Application Load Balancer and attach target group that you created in \u0026lsquo;STEP 1\u0026rsquo;  STEP 7 - Testing In your browser, browser to http://elb-endpoint/web-demo/index.php. As you can see, our demo seems to be working on multiple servers. This is so easy!\n"
},
{
	"uri": "/lab3/",
	"title": "실습3. S3 Integration",
	"tags": [],
	"description": "",
	"content": "Level 3 – Offload user loads to S3  Shared storage Reduce pressure on web server Why IAM Roles  트래픽이 증가할 경우 공유파일시스템을 통한 서비스에는 제한이 있을 수 있습니다. 이 단계에서는 스토리지를 EFS에서 S3로 이동하여 이 문제를 해결합니다.\nSTEP 1 - Create S3 bucket  It is very important that your S3 bucket does not \u0026ldquo;Block new public ACLs and uploading public objects\u0026rdquo;. You can view the configurations of your S3 bucket in the S3 console under \u0026ldquo;Permissions -\u0026gt; Block public Access Settings (Off)\u0026rdquo;.  STEP 2 - Moving the storage from EFS to S3 since your we application is limited by the capability of the shared file system   From one of the EC2 instance, edit /efs/web-demo/config.php and make some minor changes. It does not matter from which EC2 instance you make the changes, because the source files are stored on EFS. The changes will be reflected on both EC2 instances.\n  In the following configuration, $s3_bucket is the name of the S3 bucket for share storage, and $s3_baseurl is the URL pointing to the S3 endpoint in the region hosting your S3 bucket. You can also identify this end point in the S3 Console by viewing the properties of an S3 object in the S3 bucket.\n  Then we use the EFS file system to store our web application.he EFS file system:\n$storage_option = \u0026#34;s3\u0026#34;; // hd or s3 $s3_region = “ap-northeast-2\u0026#34;; $s3_bucket= \u0026#34;your_s3_bucket_name\u0026#34;; $s3_prefix= \u0026#34;uploads\u0026#34;; $s3_baseurl= \u0026#34;https://s3.ap-northeast-2.amazonaws.com/\u0026#34;;   STEP 3 - Attach IAM Role to both EC2 Instances  In order for this new setting to work, we need to attach an IAM Role to both EC2 instances. In the IAM Console, create an EC2 Role in the IAM Console, which allows full access to S3 (IAM Console -\u0026gt; Role -\u0026gt; Create Role -\u0026gt; AWS Service -\u0026gt; EC2 -\u0026gt; EC2 -\u0026gt; Amazon S3 Full Access). In the EC2 Console, attach the newly created role to both EC2 instances one by one (Instance -\u0026gt; Actions -\u0026gt; Instance Settings -\u0026gt; Attach/Replace IAM Role). In your browser, again browse to http://elb-endpoint/web-demo/index.php. At this point you will see missing images, because the previously uploaded images are not available on S3. Newly uploaded images will go to S3 instead of local disk. The reason we use IAM Role in this tutorial is that with IAM Role you do not need to supply your AWS Access Key and Secret Key in your code. Rather, Your code will assume the role assigned to the EC2 instance, and access the AWS resources that your EC2 instance is allowed to access. Today many people and organizations host their source code on github.com or some other public repositories. By using IAM roles you no longer hard code your AWS credentials in your application, thus eliminating the possibility of leaking your AWS credentials to the public.  "
},
{
	"uri": "/lab4/",
	"title": "실습4. AutoScaling",
	"tags": [],
	"description": "",
	"content": "Level 4 – Dynamically scale the size of your server fleet according to the actual traffic to your web application  Latency simulation  이번 레벨에서는 임의로 페이지 로딩 시간을 지연 시키는 모의 실험을 통해 scale in/out 동작을 확인합니다.\n모의 실험 환경 구성   If you look at /efs/web-demo/config.php, you will find this piece of code:\n$latency = 0; -\u0026gt; 1 (seconds)\n  And, in /efs/web-demo/index.php, there is a corresponding statement:\nsleep($latency);\n  오토스케이링을 이용하여 동적으로 백엔드 서버 수 조정   In your EC2 Console, create a launch configuration using the AMI and the IAM Role that we created in LEVEL 3.\n  Create an AutoScaling group using the launch configuration we created in step \u0026lsquo;1\u0026rsquo;, make sure that the AutoScaling group receives traffic from your ALB target group. Also, change the health check type from EC2 to ELB. (This way, when the ELB determines that an instance is unhealthy, the AutoScaling group will terminate it.) You don’t need to specify any scaling policy at this point.\n  Click on your ELB and create a new CloudWatch Alarm (ELB -\u0026gt; Monitoring -\u0026gt; Create Alarm) when the average latency (response time) is greater than 1 secs for at least 1 minutes.\n  Click on your AutoScaling group, and create a new simple scaling policy (AutoScaling -\u0026gt; Scaling Policies -\u0026gt; Add policy ), using the CloudWatch Alarm you just created. The auto scaling action can be “add 1 instance and then wait 300 seconds”. This way, if the average latency of your web application exceeds 1 second, AutoScaling will add one more instance to your fleet. You can do the testing by adjusting the $latency value on your existing web servers. Please note the source code resides on your EFS file system, when you make the change from one of your EC2 instances, the change is reflected on all of your EC2 instances.\nWhen you are done with this step, you can play with scaling down by creating another CloudWatch Alarm and a corresponding auto scaling policy. The CloudWatch alarm will be alarmed when the average latency is smaller than 500 ms for at least 1 minute, and the auto scaling action can be “remove 1 instance and then wait 300 seconds”.\n    sudo apt-get install nfs-common\n  sudo mkdir /efs\n  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs\n  sudo chown -R ubuntu:ubuntu /efs\n  Then we add the mounting stuff into /etc/fstab to add the following line, so that you do not need to manually mount the EFS file system when the operating system is rebooted.\n\u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs nfs auto,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0   You can verify the above-mentioned configuration is working using the following commands (run them several times):\n$ df -h $ mount $ sudo umount /efs $ sudo mount /efs   STEP 2 - Install the Apache and PHP   Run the following commands to install Apache and PHP. Notice that we are not installing the MySQL server this time.\n sudo apt-get install apache2 php mysql-client libapache2-mod-php php-mysql php-curl php-xml awscli sudo service apache2 start    Then we use the EFS file system to store our web application.he EFS file system:\n$ cd /efs $ git clone https://github.com/qyjohn/web-demo $ cd web-demo $ sudo chown -R www-data:www-data uploads $ cd /var/www/html $ sudo ln -s /efs/web-demo web-demo   STEP 3 - Launch an RDS Instance  Launch an RDS instance running MySQL. When launching the RDS instance (Dev/Test-Mysql, t2.micro), create a default database named “web_demo”. When the RDS instance becomes available. Please make sure that the security group being used on the RDS instance allows inbound connection from your EC2 instance. Then, connect to the RDS instance and create a user for your application. This time, when granting privileges, you need to grant external access for the user.  mysql -h [endpoint-of-rds-instance] -u \u0026lt;master username\u0026gt; -p mysql\u0026gt; CREATE DATABASE web_demo;* mysql\u0026gt; CREATE USER \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON web_demo.* TO \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39;; mysql\u0026gt; quit  Then, use the following command to import the demo data in web_demo.sql to the web_demo database on the RDS database:  $ cd /var/www/html/web-demo $ mysql -h [endpoint-of-rds-instance] -u username -p web_demo \u0026lt; web_demo.sql  Now, modify config.php with the new database server hostname, username, password, and database name.  STEP 4 - Create an ElastiCacheMemcached Cluster   We use ElastiCache to resolve the session sharing issue between multiple web servers. In the ElastiCache console, launch an ElastiCache cluster with Memcached (just 1 single node is enough) and obtain the endpoint information. Please make sure that the security group being used on the ElastiCache cluster allows inbound connection from your EC2 instance.\n  On the web server, configure php.ini to use Memcached for session sharing.\n  Edit /etc/php/7.2/apache2/php.ini. Make the following modifications:\nsession.save_handler = memcached session.save_path = \u0026#34;[dns-endpoint-to-the-elasticache-node]:11211\u0026#34;   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcached $ sudo service apache2 restart   Edit /etc/php/7.2/mods-available/memcached.ini, add the following two lines to support session redundancy. Please note that the value of memcache.session_redundancy equals to the number of cache nodes plus 1 (because of a bug in PHP).\nmemcache.allow_failover=1 memcache.session_redundancy=2   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcache $ sudo service apache2 restart   STEP 5 - Create an AMI Now, create an AMI from the EC2 instance and launch a new EC2 instance with the AMI.\nSTEP 6 - Create an ELB Create an Application Load Balancer (ALB) and register the two EC2 instances to the ALB target group. Since we do have Apache running on both web servers, you might want to use HTTP as the ping protocol with 80 as the ping port and “/” as the ping path for the health check parameter for your ELB.\n Create Target Group and Register 2 EC2 instances that you created before Create Application Load Balancer and attach target group that you created in \u0026lsquo;STEP 1\u0026rsquo;  STEP 7 - Testing In your browser, browser to http://elb-endpoint/web-demo/index.php. As you can see, our demo seems to be working on multiple servers. This is so easy!\n"
},
{
	"uri": "/lab5/",
	"title": "실습5. DB Cache",
	"tags": [],
	"description": "",
	"content": "Level 5 – Implement a cache layer between the web server and the database server  Reduce DB Pressure Reuse ElastiCache node  이번 레벨에서는 ElastiCache 를 통해 database를 캐싱 하도록 데모 코드를 수정하고 캐싱 동작을 확인합니다.\nDatabase caching through ElastiCache   This caching behavior is not enable by default. You can edit config.php on all web servers with details regarding the cache server:\n  in config.php, there is a corresponding statement :\n// Cache configuration $enable_cache = true; $cache_type = \u0026#34;memcached\u0026#34;;\t// memcached or redis $cache_key = \u0026#34;images_html\u0026#34;; if ($enable_cache \u0026amp;\u0026amp; ($cache_type == \u0026#34;memcached\u0026#34;)) { $cache = open_memcache_connection(); } else if ($enable_cache \u0026amp;\u0026amp; ($cache_type == \u0026#34;redis\u0026#34;)) { $cache = open_redis_connection(); }   Use AutoScaling to scale your server fleet in a dynamic fashion Refresh the demo application in your browser, you will see that the “Getting latest N records from database.” message is now gone, indicating that the information you are seeing is obtained from ElastiCache. When you upload a new image, you will see this message again, indicating the cache is being updated.\n  The following code is responsible of handling this cache logic:\n// Get the most recent N images if ($enable_cache) { // Attemp to get the cached records for the front page  $images_html = $cache-\u0026gt;get($cache_key); if (!$images_html) { // If there is no such cached record, get it from the database  $images = retrieve_recent_uploads($db, 10, $storage_option); // Convert the records into HTML  $images_html = db_rows_2_html($images, $storage_option, $hd_folder, $s3_bucket, $s3_baseurl); // Then put the HTML into cache  $cache-\u0026gt;set($cache_key, $images_html); } } else { // This statement get the last 10 records from the database  $images = retrieve_recent_uploads($db, 10, $storage_option); $images_html = db_rows_2_html($images, $storage_option, $hd_folder, $s3_bucket, $s3_baseurl); } // Display the images echo $images_html;   Also pay attention to this code when doing image uploads. We deleted the cache after the user uploads an images. This way, when the next request comes in, we will fetch the latest records from the database, and put them into the cache again.\nif ($enable_cache) { // Delete the cached record, the user will query the database to get an updated version  if ($cache_type == \u0026#34;memcached\u0026#34;) { $cache-\u0026gt;delete($cache_key); } else if ($cache_type == \u0026#34;redis\u0026#34;) { $cache-\u0026gt;del($cache_key); } }   "
},
{
	"uri": "/lab6/",
	"title": "실습6. CDN Integration",
	"tags": [],
	"description": "",
	"content": "Level 6 – Use CloudFront for content delivery  Use CloudFront Reduce S3 pressure Serve from edge locations  Create CloudFront distribution   In this level, we create a CloudFront distribution with your S3 bucket as the origin. This way your static content is served to your end users from the nearest edge locations. In your code, you only need to make the following tiny changes.\n$enable_cf = true; $cf_baseurl = “http://xxxxxxxxxxxx.cloudfront.net\u0026#34;;   Reload the web page in your browser to observe the behavior. Are you able to use CloudFront when the uploaded pictures are stored on disk?\n  "
},
{
	"uri": "/lab7/",
	"title": "실습7. Log Analysis",
	"tags": [],
	"description": "",
	"content": "Level 7 – Use Kinesis Analytics to perform simple near realtime analysis for your web traffic.  Kinesis data stream Kinesis Analytics  Log Analysis   In this level, we will look into how we can perform real-time log analysis for your web application. This is achieve using the Kinesis data stream and Kinesis Analytics application.\n  First of all, we need to create two Kinesis data streams (using the Kinesis web console) in the us-east-1 region: web-access-log (1 shard is sufficient for our demo).\n  SSH into your EC2 instance, configure your Apache to log in JSON format. This will make it easier for Kinesis Analytics to work with your logs. Edit /etc/apache2/apache2.conf, find the area with LogFormat, and add the following new log format to it. For more information on custom log format for Apache, please refer to Apache Module mod_log_config.\nLogFormat \u0026#34;{ \\\u0026#34;request_time\\\u0026#34;:\\\u0026#34;%t\\\u0026#34;, \\\u0026#34;client_ip\\\u0026#34;:\\\u0026#34;%a\\\u0026#34;, \\\u0026#34;client_hostname\\\u0026#34;:\\\u0026#34;%V\\\u0026#34;, \\\u0026#34;server_ip\\\u0026#34;:\\\u0026#34;%A\\\u0026#34;, \\\u0026#34;request\\\u0026#34;:\\\u0026#34;%U\\\u0026#34;, \\\u0026#34;http_method\\\u0026#34;:\\\u0026#34;%m\\\u0026#34;, \\\u0026#34;status\\\u0026#34;:\\\u0026#34;%\u0026gt;s\\\u0026#34;, \\\u0026#34;size\\\u0026#34;:\\\u0026#34;%B\\\u0026#34;, \\\u0026#34;userAgent\\\u0026#34;:\\\u0026#34;%{User-agent}i\\\u0026#34;, \\\u0026#34;referer\\\u0026#34;:\\\u0026#34;%{Referer}i\\\u0026#34; }\u0026#34; kinesis   Then edit /etc/apache2/sites-available/000-default.conf, change the CustomLog line to use your own log format:\nCustomLog ${APACHE_LOG_DIR}/access.log kinesis   Restart Apache to allow the new configuration to take effect:\n$ sudo service apache2 restart\n  Check Apache Accesslog to see if the Logformat applied:\n$ tail –f /var/log/apache2/access.log\n  Install Kinesis agent   Then, install and configure the Kinesis Agent\n$ cd ~ $ sudo apt-get install openjdk-8-jdk $ git clone https://github.com/awslabs/amazon-kinesis-agent $ cd amazon-kinesis-agent $ sudo ./setup --install   After the agent is installed, the configuration file can be found in /etc/aws-kinesis/agent.json. Edit the configuration file to send your Apache access log to the web-access-log stream. (Let\u0026rsquo;s not worry about the error log in this tutorial.)\n{ \u0026#34;cloudwatch.emitMetrics\u0026#34;: true, \u0026#34;kinesis.endpoint\u0026#34;: \u0026#34;kinesis.ap-northeast-2.amazonaws.com\u0026#34;, \u0026#34;firehose.endpoint\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;flows\u0026#34;: [ { \u0026#34;filePattern\u0026#34;: \u0026#34;/var/log/apache2/access.log\u0026#34;, \u0026#34;kinesisStream\u0026#34;: \u0026#34;web-access-log\u0026#34;, \u0026#34;partitionKeyOption\u0026#34;: \u0026#34;RANDOM\u0026#34; } ] }   Once you updated the configuration file, you can start the Kinesis Agent using the following command:\n$ sudo service aws-kinesis-agent stop\n$ sudo service aws-kinesis-agent start\n  Then you can check the status of the Kinesis Agent using the following command:\n$ sudo service aws-kinesis-agent status\n  If the agent is not working as expected, look into the logs (under /var/log/aws-kinesis-agent) to understand what is going on. (If there is no log, what would you do?) It is likely that the user running the Kinesis Agent (aws-kinesis-agent-user) does not have access to the Apache logs (/var/log/apache2/).\n  To resolve this issue, you can add the aws-kinesis-agent-user to the adm group.\n$ sudo usermod -a -G adm aws-kinesis-agent-user $ sudo service aws-kinesis-agent stop $ sudo service aws-kinesis-agent start   Refresh your web application in the browser, then watch the Kinesis Agent logs to see whether your logs are pushed to the Kinesis streams. When the Kinesis Agent says the logs are successfully sent to destinations, check the \u0026ldquo;Monitoring\u0026rdquo; tab in the Kinesis data streams console to confirm this.\n  Create a new AMI from the above-mentioned EC2 instance, then create a new launch configuration from the new AMI. Modify your Auto Scaling group to use the new launch configuration. This way, all of the EC2 instance in your web server fleet is capable of sending logs to your Kinesis stream.\n  Now go to the Kinesis Analytics console to create a Kinesis Analytics Application, with the web-access-log data stream as the source. Click on the \u0026ldquo;Discover scheme\u0026rdquo; to automatically discover the scheme in the data, then save the scheme and continue. In the SQL Editor, copy and paste the following sample SQL statements. Then click on the \u0026ldquo;Save and run SQL\u0026rdquo; button to start your application.\n-- Create a destination stream CREATE OR REPLACE STREAM \u0026#34;DESTINATION_SQL_STREAM\u0026#34; (client_ip VARCHAR(16), request_count INTEGER); -- Create a pump which continuously selects from a source stream (SOURCE_SQL_STREAM_001) CREATE OR REPLACE PUMP \u0026#34;STREAM_PUMP\u0026#34; AS INSERT INTO \u0026#34;DESTINATION_SQL_STREAM\u0026#34; -- Aggregation functions COUNT|AVG|MAX|MIN|SUM|STDDEV_POP|STDDEV_SAMP|VAR_POP|VAR_SAMP SELECT STREAM \u0026#34;client_ip\u0026#34;, COUNT(*) AS request_count FROM \u0026#34;SOURCE_SQL_STREAM_001\u0026#34; -- Uses a 10-second tumbling time window GROUP BY \u0026#34;client_ip\u0026#34;, FLOOR((\u0026#34;SOURCE_SQL_STREAM_001\u0026#34;.ROWTIME - TIMESTAMP \u0026#39;1970-01-01 00:00:00\u0026#39;) SECOND / 10 TO SECOND);   From multiple EC2 instances, use the Apache Benchmark tool (ab) to generate some more web traffic. Observe and explain the query results in the Kinesis Analytics console.\n$ ab -n 100000 -c 2 http://\u0026lt;dns-endpoint-of-your-load-balancer\u0026gt;/web-demo/index.php   Secreenshot on Kinesis Analytics "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]