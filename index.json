[
{
	"uri": "/",
	"title": "í™ˆ",
	"tags": [],
	"description": "",
	"content": "Scalable Image Upload Web Application  ì´ ì‹¤ìŠµì—ì„œëŠ” ë‹¤ì–‘í•œ AWS ë¹Œë”© ë¸”ë¡ (EC2, ELB, RDS, ElastiCache, S3, IAM, CloudWatch, AutoScaling ë“±)ì„ ì‚¬ìš©í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ë‚´ìš©ì„ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.\nì£¼ìš” ì„œë¹„ìŠ¤ êµ¬ì„±ìš”ì†Œ  EC2 ELB AutoScaling RDS Elasticache S3 IAM  ë°ëª¨ ì›¹í˜ì´ì§€ ìµœì¢… ì„œë¹„ìŠ¤ êµ¬ì„±ë„  "
},
{
	"uri": "/lab1/",
	"title": "ì‹¤ìŠµ1. Basic Version",
	"tags": [],
	"description": "",
	"content": "Level 1 â€“ Basic Version (All in one)  Single Server Implementation Web Frontend (Apache + PHP) DB Backend (MYSQL) Upload to local storage  ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘  ì´ ë©ì—ì„œëŠ” ë‹¨ì¼ ì„œë²„ ìœ„ì— ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ë™ì— í•„ìš”í•œ ì›¹ì„œë²„í”„ë¡œê·¸ë¨, ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ê·¸ë¦¬ê³  í”„ë¡œê·¸ë˜ë°ì— í•„ìš”í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ê°œë°œí•œ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ë°°í¬í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤. í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ë‹¨ì¼ ì„œë²„ëŠ” EC2 ì´ê³  ì›¹ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ë™ì— í•„ìš”í•œ ì†Œí”„íŠ¸ì›¨ì–´ëŠ” Apache, PHP, MySQL ì…ë‹ˆë‹¤. EC2 ì¸ìŠ¤í„´ìŠ¤ë¡œ ì‚¬ìš©í•  ì´ë¯¸ì§€(AMI)ëŠ” Ubuntu 18.04 AMI ì…ë‹ˆë‹¤.   EC2 ìƒì„±ì— í•„ìš”í•œ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ì‹œê³  ì›¹ì„œë¹„ìŠ¤ì™€ SSH ì ‘ì†ì„ ìœ„í•´ 80ë²ˆê³¼ 22ë²ˆ í¬íŠ¸ë¥¼ ì—´ì–´ì¤ë‹ˆë‹¤. EC2 ìƒì„±ì´ ì™„ë£Œëœ í›„ì—ëŠ” SSH ë¡œ ì ‘ì†í•˜ì—¬ MySQL, PHP, Apache ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì„¤ì¹˜í•˜ê³  git ëª…ë ¹ì„ í†µí•´ ì›¹ì• í”Œë¦¬ì¼€ì´ì…˜ ë°ëª¨ë¥¼ ìœ„í•œ ê°œë°œì½”ë“œë¥¼ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤.  ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ MySQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì¹˜\nsudo apt-get update\nsudo apt-get install mysql-server\nsudo mysql_secure_installation\nğŸ‘‰ type \u0026lsquo;Y\u0026rsquo; for followed question\nApache ì™€ PHP ì„¤ì¹˜\nsudo apt-get install apache2 php libapache2-mod-php php-mysql php-curl php-xml php-memcached awscli Apache ì„œë²„ ì¬ì‹œì‘\nsudo service apache2 restart git ì„¤ì¹˜ ë° ë°ëª¨ìš© ì›¹ì• í”Œë¦¬ì¼€ì´ì…˜ ë‹¤ìš´ë¡œë“œ\nsudo apt-get install git cd /var sudo chown -R ubuntu:ubuntu www cd /var/www/html git clone https://github.com/qyjohn/web-demo cd web-demo ì›¹ì„œë¹„ìŠ¤ìš© ë””ë ‰í† ë¦¬ ê¶Œí•œ ë³€ê²½\n sudo chown -R www-data:www-data uploads* ë°ëª¨ìš© ì›¹ì• í”Œë¦¬ì¼€ì´ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\nsudo mysql mysql\\\u0026gt; CREATE DATABASE web_demo; mysql\\\u0026gt; CREATE USER \\'username\\'@\\'localhost\\' IDENTIFIED BY 'password\\'; mysql\\\u0026gt; GRANT ALL PRIVILEGES ON web_demo.\\* TO 'username\\'@\\'localhost\\'; mysql\\\u0026gt; quit ë°ëª¨ìš© ë°ì´í„° ì¶”ê°€\n ê°œë°œì†ŒìŠ¤ì½”ë“œì˜ Database ì ‘ì†ì„ ìœ„í•œ ë°ì´í„°ë² ì´ìŠ¤ì´ë¦„, ì‚¬ìš©ì ì´ë¦„ê³¼ ë¹„ë°€ë²ˆí˜¸ëŠ” ê°ê°â€œweb_demoâ€, â€œusernameâ€ ê·¸ë¦¬ê³  â€œpasswordâ€ ì…ë‹ˆë‹¤. * git ëª…ë ¹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë°›ì€ ì†ŒìŠ¤ì½”ë“œ ë‚´ì—ëŠ” ë¯¸ë¦¬ ì…ë ¥ëœ web_demo.sql ë¼ëŠ” SQL ë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ ì´ë¥¼ import í•˜ì—¬Databaseì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. git ëª…ë ¹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë°›ì€ ì†ŒìŠ¤ì½”ë“œ ë‚´ì—ëŠ” ë¯¸ë¦¬ ì…ë ¥ëœ web_demo.sql ë¼ëŠ” SQL ë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ ì´ë¥¼ import í•˜ì—¬Databaseì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.  cd /var/www/html/web-demo mysql -u username -p web_demo \u0026lt; web_demo.sql ì£¼ì˜ì‚¬í•­\n ë§Œì•½ ë°ì´í„°ë² ì´ìŠ¤ì´ë¦„, ì‚¬ìš©ìì´ë¦„, ë¹„ë°€ë²ˆí˜¸ë¥¼ ë³€ê²½í•˜ê³  ì‹¶ì„ ê²½ìš° vi ì—ë””í„°ë¥¼ ì´ìš©í•˜ì—¬config.php ì˜ ë‚´ìš©ì„ ì§ì ‘ ìˆ˜ì •í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. PC ì— ìˆëŠ” ì•„ë¬´ ì¸í„°ë„·ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ì— http://ip-address/web-demo/index.php ë¥¼ ì…ë ¥í•˜ì—¬ ë°ëª¨ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í™•ì¸í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. ê¸°íƒ€ ë°ëª¨ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ê´€ë ¨ëœ ì œì•½ ì‚¬í•­ì€ ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.   You can login with your name, then upload some photos for testing. (You might have noticed that this demo application does not ask you for a password. This is because we would like to make things as simple as possible. Handling user password is a very complicate issue, which is beyond the scope of this entry level tutorial.) Then I suggest that you spend 10 minutes reading through the demo code index.php. The demo code has reasonable documentation in the form of comments, so I am not going to explain the code here.\n  Now you are able to get your website working, please upload some more pictures for testing. Upload some small pictures and some big pictures (like 20 MB) to see what happens. Fix any issues you may observe in the tests.\n "
},
{
	"uri": "/lab2/",
	"title": "ì‹¤ìŠµ2. Load Balanced Solution",
	"tags": [],
	"description": "",
	"content": "Level 2 â€“ Scale the application to two or more servers  Session sharing issue (ELB or Cache ?) Shared database issue (RDS) Shared Storage issue (EFS) NFS? Single point of failure  ì´ë²ˆ ë ˆë²¨ì—ì„œëŠ” ì‹¤ìŠµ1 ì˜ ê¸°ë³¸ë²„ì „ì„ í™•ì¥í•˜ì—¬ ì—¬ëŸ¬ ì„œë²„ì— ë°°í¬í•˜ê³ ì í•©ë‹ˆë‹¤. íŒŒì¼ ê³µìœ ë¥¼ ìœ„í•´ NFSë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ (Amazon) EFSë¥¼ ì‚¬ìš©í•˜ë©´ ì‰½ê²Œ íŒŒì¼ ê³µìœ  ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nSTEP 1 - EFS File System ì¤€ë¹„   Go to the EFS Console and create an EFS file system. (Note the DNS Name for EFS file system. You need this value for the next step)\n  Terminate the previous EC2 instance because we no longer need it. Launch a new EC2 instance with the Ubuntu 18.04 operating system. SSH into the EC2 instance to install the following software and mount the EFS file system:\n sudo apt-get update sudo apt-get install nfs-common sudo mkdir /efs sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs sudo chown -R ubuntu:ubuntu /efs    Then we add the mounting stuff into /etc/fstab to add the following line, so that you do not need to manually mount the EFS file system when the operating system is rebooted.\n\u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs nfs auto,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0   You can verify the above-mentioned configuration is working using the following commands (run them several times):\n$ df -h $ mount $ sudo umount /efs $ sudo mount /efs   STEP 2 - Install the Apache and PHP   Run the following commands to install Apache and PHP. Notice that we are not installing the MySQL server this time.\n sudo apt-get install apache2 php mysql-client libapache2-mod-php php-mysql php-curl php-xml awscli sudo service apache2 start    Then we use the EFS file system to store our web application.he EFS file system:\n$ cd /efs $ git clone https://github.com/qyjohn/web-demo $ cd web-demo $ sudo chown -R www-data:www-data uploads $ cd /var/www/html $ sudo ln -s /efs/web-demo web-demo   STEP 3 - Launch an RDS Instance  Launch an RDS instance running MySQL. When launching the RDS instance (Dev/Test-Mysql, t2.micro), create a default database named â€œweb_demoâ€. When the RDS instance becomes available. Please make sure that the security group being used on the RDS instance allows inbound connection from your EC2 instance. Then, connect to the RDS instance and create a user for your application. This time, when granting privileges, you need to grant external access for the user.  mysql -h [endpoint-of-rds-instance] -u \u0026lt;master username\u0026gt; -p mysql\u0026gt; CREATE DATABASE web_demo;* mysql\u0026gt; CREATE USER \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON web_demo.* TO \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39;; mysql\u0026gt; quit  Then, use the following command to import the demo data in web_demo.sql to the web_demo database on the RDS database:  $ cd /var/www/html/web-demo $ mysql -h [endpoint-of-rds-instance] -u username -p web_demo \u0026lt; web_demo.sql  Now, modify config.php with the new database server hostname, username, password, and database name.  STEP 4 - Create an ElastiCacheMemcached Cluster   We use ElastiCache to resolve the session sharing issue between multiple web servers. In the ElastiCache console, launch an ElastiCache cluster with Memcached (just 1 single node is enough) and obtain the endpoint information. Please make sure that the security group being used on the ElastiCache cluster allows inbound connection from your EC2 instance.\n  On the web server, configure php.ini to use Memcached for session sharing.\n  Edit /etc/php/7.2/apache2/php.ini. Make the following modifications:\nsession.save_handler = memcached session.save_path = \u0026#34;[dns-endpoint-to-the-elasticache-node]:11211\u0026#34;   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcached $ sudo service apache2 restart   Edit /etc/php/7.2/mods-available/memcached.ini, add the following two lines to support session redundancy. Please note that the value of memcache.session_redundancy equals to the number of cache nodes plus 1 (because of a bug in PHP).\nmemcache.allow_failover=1 memcache.session_redundancy=2   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcache $ sudo service apache2 restart   STEP 5 - Create an AMI Now, create an AMI from the EC2 instance and launch a new EC2 instance with the AMI.\nSTEP 6 - Create an ELB Create an Application Load Balancer (ALB) and register the two EC2 instances to the ALB target group. Since we do have Apache running on both web servers, you might want to use HTTP as the ping protocol with 80 as the ping port and â€œ/â€ as the ping path for the health check parameter for your ELB.\n Create Target Group and Register 2 EC2 instances that you created before Create Application Load Balancer and attach target group that you created in \u0026lsquo;STEP 1\u0026rsquo;  STEP 7 - Testing In your browser, browser to http://elb-endpoint/web-demo/index.php. As you can see, our demo seems to be working on multiple servers. This is so easy!\n"
},
{
	"uri": "/lab3/",
	"title": "ì‹¤ìŠµ3. S3 Integration",
	"tags": [],
	"description": "",
	"content": "Level 3 â€“ Offload user loads to S3  Shared storage Reduce pressure on web server Why IAM Roles  íŠ¸ë˜í”½ì´ ì¦ê°€í•  ê²½ìš° ê³µìœ íŒŒì¼ì‹œìŠ¤í…œì„ í†µí•œ ì„œë¹„ìŠ¤ì—ëŠ” ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ìŠ¤í† ë¦¬ì§€ë¥¼ EFSì—ì„œ S3ë¡œ ì´ë™í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.\nSTEP 1 - Create S3 bucket  It is very important that your S3 bucket does not \u0026ldquo;Block new public ACLs and uploading public objects\u0026rdquo;. You can view the configurations of your S3 bucket in the S3 console under \u0026ldquo;Permissions -\u0026gt; Block public Access Settings (Off)\u0026rdquo;.  STEP 2 - Moving the storage from EFS to S3 since your we application is limited by the capability of the shared file system   From one of the EC2 instance, edit /efs/web-demo/config.php and make some minor changes. It does not matter from which EC2 instance you make the changes, because the source files are stored on EFS. The changes will be reflected on both EC2 instances.\n  In the following configuration, $s3_bucket is the name of the S3 bucket for share storage, and $s3_baseurl is the URL pointing to the S3 endpoint in the region hosting your S3 bucket. You can also identify this end point in the S3 Console by viewing the properties of an S3 object in the S3 bucket.\n  Then we use the EFS file system to store our web application.he EFS file system:\n$storage_option = \u0026#34;s3\u0026#34;; // hd or s3 $s3_region = â€œap-northeast-2\u0026#34;; $s3_bucket= \u0026#34;your_s3_bucket_name\u0026#34;; $s3_prefix= \u0026#34;uploads\u0026#34;; $s3_baseurl= \u0026#34;https://s3.ap-northeast-2.amazonaws.com/\u0026#34;;   STEP 3 - Attach IAM Role to both EC2 Instances  In order for this new setting to work, we need to attach an IAM Role to both EC2 instances. In the IAM Console, create an EC2 Role in the IAM Console, which allows full access to S3 (IAM Console -\u0026gt; Role -\u0026gt; Create Role -\u0026gt; AWS Service -\u0026gt; EC2 -\u0026gt; EC2 -\u0026gt; Amazon S3 Full Access). In the EC2 Console, attach the newly created role to both EC2 instances one by one (Instance -\u0026gt; Actions -\u0026gt; Instance Settings -\u0026gt; Attach/Replace IAM Role). In your browser, again browse to http://elb-endpoint/web-demo/index.php. At this point you will see missing images, because the previously uploaded images are not available on S3. Newly uploaded images will go to S3 instead of local disk. The reason we use IAM Role in this tutorial is that with IAM Role you do not need to supply your AWS Access Key and Secret Key in your code. Rather, Your code will assume the role assigned to the EC2 instance, and access the AWS resources that your EC2 instance is allowed to access. Today many people and organizations host their source code on github.com or some other public repositories. By using IAM roles you no longer hard code your AWS credentials in your application, thus eliminating the possibility of leaking your AWS credentials to the public.  "
},
{
	"uri": "/lab4/",
	"title": "ì‹¤ìŠµ4. AutoScaling",
	"tags": [],
	"description": "",
	"content": "Level 4 â€“ Dynamically scale the size of your server fleet according to the actual traffic to your web application  Latency simulation  ì´ë²ˆ ë ˆë²¨ì—ì„œëŠ” ì„ì˜ë¡œ í˜ì´ì§€ ë¡œë”© ì‹œê°„ì„ ì§€ì—° ì‹œí‚¤ëŠ” ëª¨ì˜ ì‹¤í—˜ì„ í†µí•´ scale in/out ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤.\nëª¨ì˜ ì‹¤í—˜ í™˜ê²½ êµ¬ì„±   If you look at /efs/web-demo/config.php, you will find this piece of code:\n$latency = 0; -\u0026gt; 1 (seconds)\n  And, in /efs/web-demo/index.php, there is a corresponding statement:\nsleep($latency);\n  ì˜¤í† ìŠ¤ì¼€ì´ë§ì„ ì´ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ë°±ì—”ë“œ ì„œë²„ ìˆ˜ ì¡°ì •   In your EC2 Console, create a launch configuration using the AMI and the IAM Role that we created in LEVEL 3.\n  Create an AutoScaling group using the launch configuration we created in step \u0026lsquo;1\u0026rsquo;, make sure that the AutoScaling group receives traffic from your ALB target group. Also, change the health check type from EC2 to ELB. (This way, when the ELB determines that an instance is unhealthy, the AutoScaling group will terminate it.) You donâ€™t need to specify any scaling policy at this point.\n  Click on your ELB and create a new CloudWatch Alarm (ELB -\u0026gt; Monitoring -\u0026gt; Create Alarm) when the average latency (response time) is greater than 1 secs for at least 1 minutes.\n  Click on your AutoScaling group, and create a new simple scaling policy (AutoScaling -\u0026gt; Scaling Policies -\u0026gt; Add policy ), using the CloudWatch Alarm you just created. The auto scaling action can be â€œadd 1 instance and then wait 300 secondsâ€. This way, if the average latency of your web application exceeds 1 second, AutoScaling will add one more instance to your fleet. You can do the testing by adjusting the $latency value on your existing web servers. Please note the source code resides on your EFS file system, when you make the change from one of your EC2 instances, the change is reflected on all of your EC2 instances.\nWhen you are done with this step, you can play with scaling down by creating another CloudWatch Alarm and a corresponding auto scaling policy. The CloudWatch alarm will be alarmed when the average latency is smaller than 500 ms for at least 1 minute, and the auto scaling action can be â€œremove 1 instance and then wait 300 secondsâ€.\n    sudo apt-get install nfs-common\n  sudo mkdir /efs\n  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs\n  sudo chown -R ubuntu:ubuntu /efs\n  Then we add the mounting stuff into /etc/fstab to add the following line, so that you do not need to manually mount the EFS file system when the operating system is rebooted.\n\u0026lt;dns-endpoint-of-your-efs-file-system\u0026gt;:/ /efs nfs auto,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0   You can verify the above-mentioned configuration is working using the following commands (run them several times):\n$ df -h $ mount $ sudo umount /efs $ sudo mount /efs   STEP 2 - Install the Apache and PHP   Run the following commands to install Apache and PHP. Notice that we are not installing the MySQL server this time.\n sudo apt-get install apache2 php mysql-client libapache2-mod-php php-mysql php-curl php-xml awscli sudo service apache2 start    Then we use the EFS file system to store our web application.he EFS file system:\n$ cd /efs $ git clone https://github.com/qyjohn/web-demo $ cd web-demo $ sudo chown -R www-data:www-data uploads $ cd /var/www/html $ sudo ln -s /efs/web-demo web-demo   STEP 3 - Launch an RDS Instance  Launch an RDS instance running MySQL. When launching the RDS instance (Dev/Test-Mysql, t2.micro), create a default database named â€œweb_demoâ€. When the RDS instance becomes available. Please make sure that the security group being used on the RDS instance allows inbound connection from your EC2 instance. Then, connect to the RDS instance and create a user for your application. This time, when granting privileges, you need to grant external access for the user.  mysql -h [endpoint-of-rds-instance] -u \u0026lt;master username\u0026gt; -p mysql\u0026gt; CREATE DATABASE web_demo;* mysql\u0026gt; CREATE USER \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON web_demo.* TO \u0026#39;username\u0026#39;@\u0026#39;%\u0026#39;; mysql\u0026gt; quit  Then, use the following command to import the demo data in web_demo.sql to the web_demo database on the RDS database:  $ cd /var/www/html/web-demo $ mysql -h [endpoint-of-rds-instance] -u username -p web_demo \u0026lt; web_demo.sql  Now, modify config.php with the new database server hostname, username, password, and database name.  STEP 4 - Create an ElastiCacheMemcached Cluster   We use ElastiCache to resolve the session sharing issue between multiple web servers. In the ElastiCache console, launch an ElastiCache cluster with Memcached (just 1 single node is enough) and obtain the endpoint information. Please make sure that the security group being used on the ElastiCache cluster allows inbound connection from your EC2 instance.\n  On the web server, configure php.ini to use Memcached for session sharing.\n  Edit /etc/php/7.2/apache2/php.ini. Make the following modifications:\nsession.save_handler = memcached session.save_path = \u0026#34;[dns-endpoint-to-the-elasticache-node]:11211\u0026#34;   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcached $ sudo service apache2 restart   Edit /etc/php/7.2/mods-available/memcached.ini, add the following two lines to support session redundancy. Please note that the value of memcache.session_redundancy equals to the number of cache nodes plus 1 (because of a bug in PHP).\nmemcache.allow_failover=1 memcache.session_redundancy=2   Then you need to restart Apache the web server to make the new configuration effective.\n$ sudo apt-get install php-memcache $ sudo service apache2 restart   STEP 5 - Create an AMI Now, create an AMI from the EC2 instance and launch a new EC2 instance with the AMI.\nSTEP 6 - Create an ELB Create an Application Load Balancer (ALB) and register the two EC2 instances to the ALB target group. Since we do have Apache running on both web servers, you might want to use HTTP as the ping protocol with 80 as the ping port and â€œ/â€ as the ping path for the health check parameter for your ELB.\n Create Target Group and Register 2 EC2 instances that you created before Create Application Load Balancer and attach target group that you created in \u0026lsquo;STEP 1\u0026rsquo;  STEP 7 - Testing In your browser, browser to http://elb-endpoint/web-demo/index.php. As you can see, our demo seems to be working on multiple servers. This is so easy!\n"
},
{
	"uri": "/lab5/",
	"title": "ì‹¤ìŠµ5. DB Cache",
	"tags": [],
	"description": "",
	"content": "Level 5 â€“ Implement a cache layer between the web server and the database server  Reduce DB Pressure Reuse ElastiCache node  ì´ë²ˆ ë ˆë²¨ì—ì„œëŠ” ElastiCache ë¥¼ í†µí•´ databaseë¥¼ ìºì‹± í•˜ë„ë¡ ë°ëª¨ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  ìºì‹± ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤.\nDatabase caching through ElastiCache   This caching behavior is not enable by default. You can edit config.php on all web servers with details regarding the cache server:\n  in config.php, there is a corresponding statement :\n// Cache configuration $enable_cache = true; $cache_type = \u0026#34;memcached\u0026#34;;\t// memcached or redis $cache_key = \u0026#34;images_html\u0026#34;; if ($enable_cache \u0026amp;\u0026amp; ($cache_type == \u0026#34;memcached\u0026#34;)) { $cache = open_memcache_connection(); } else if ($enable_cache \u0026amp;\u0026amp; ($cache_type == \u0026#34;redis\u0026#34;)) { $cache = open_redis_connection(); }   Use AutoScaling to scale your server fleet in a dynamic fashion Refresh the demo application in your browser, you will see that the â€œGetting latest N records from database.â€ message is now gone, indicating that the information you are seeing is obtained from ElastiCache. When you upload a new image, you will see this message again, indicating the cache is being updated.\n  The following code is responsible of handling this cache logic:\n// Get the most recent N images if ($enable_cache) { // Attemp to get the cached records for the front page  $images_html = $cache-\u0026gt;get($cache_key); if (!$images_html) { // If there is no such cached record, get it from the database  $images = retrieve_recent_uploads($db, 10, $storage_option); // Convert the records into HTML  $images_html = db_rows_2_html($images, $storage_option, $hd_folder, $s3_bucket, $s3_baseurl); // Then put the HTML into cache  $cache-\u0026gt;set($cache_key, $images_html); } } else { // This statement get the last 10 records from the database  $images = retrieve_recent_uploads($db, 10, $storage_option); $images_html = db_rows_2_html($images, $storage_option, $hd_folder, $s3_bucket, $s3_baseurl); } // Display the images echo $images_html;   Also pay attention to this code when doing image uploads. We deleted the cache after the user uploads an images. This way, when the next request comes in, we will fetch the latest records from the database, and put them into the cache again.\nif ($enable_cache) { // Delete the cached record, the user will query the database to get an updated version  if ($cache_type == \u0026#34;memcached\u0026#34;) { $cache-\u0026gt;delete($cache_key); } else if ($cache_type == \u0026#34;redis\u0026#34;) { $cache-\u0026gt;del($cache_key); } }   "
},
{
	"uri": "/lab6/",
	"title": "ì‹¤ìŠµ6. CDN Integration",
	"tags": [],
	"description": "",
	"content": "Level 6 â€“ Use CloudFront for content delivery  Use CloudFront Reduce S3 pressure Serve from edge locations  Create CloudFront distribution   In this level, we create a CloudFront distribution with your S3 bucket as the origin. This way your static content is served to your end users from the nearest edge locations. In your code, you only need to make the following tiny changes.\n$enable_cf = true; $cf_baseurl = â€œhttp://xxxxxxxxxxxx.cloudfront.net\u0026#34;;   Reload the web page in your browser to observe the behavior. Are you able to use CloudFront when the uploaded pictures are stored on disk?\n  "
},
{
	"uri": "/lab7/",
	"title": "ì‹¤ìŠµ7. Log Analysis",
	"tags": [],
	"description": "",
	"content": "Level 7 â€“ Use Kinesis Analytics to perform simple near realtime analysis for your web traffic.  Kinesis data stream Kinesis Analytics  Log Analysis   In this level, we will look into how we can perform real-time log analysis for your web application. This is achieve using the Kinesis data stream and Kinesis Analytics application.\n  First of all, we need to create two Kinesis data streams (using the Kinesis web console) in the us-east-1 region: web-access-log (1 shard is sufficient for our demo).\n  SSH into your EC2 instance, configure your Apache to log in JSON format. This will make it easier for Kinesis Analytics to work with your logs. Edit /etc/apache2/apache2.conf, find the area with LogFormat, and add the following new log format to it. For more information on custom log format for Apache, please refer to Apache Module mod_log_config.\nLogFormat \u0026#34;{ \\\u0026#34;request_time\\\u0026#34;:\\\u0026#34;%t\\\u0026#34;, \\\u0026#34;client_ip\\\u0026#34;:\\\u0026#34;%a\\\u0026#34;, \\\u0026#34;client_hostname\\\u0026#34;:\\\u0026#34;%V\\\u0026#34;, \\\u0026#34;server_ip\\\u0026#34;:\\\u0026#34;%A\\\u0026#34;, \\\u0026#34;request\\\u0026#34;:\\\u0026#34;%U\\\u0026#34;, \\\u0026#34;http_method\\\u0026#34;:\\\u0026#34;%m\\\u0026#34;, \\\u0026#34;status\\\u0026#34;:\\\u0026#34;%\u0026gt;s\\\u0026#34;, \\\u0026#34;size\\\u0026#34;:\\\u0026#34;%B\\\u0026#34;, \\\u0026#34;userAgent\\\u0026#34;:\\\u0026#34;%{User-agent}i\\\u0026#34;, \\\u0026#34;referer\\\u0026#34;:\\\u0026#34;%{Referer}i\\\u0026#34; }\u0026#34; kinesis   Then edit /etc/apache2/sites-available/000-default.conf, change the CustomLog line to use your own log format:\nCustomLog ${APACHE_LOG_DIR}/access.log kinesis   Restart Apache to allow the new configuration to take effect:\n$ sudo service apache2 restart\n  Check Apache Accesslog to see if the Logformat applied:\n$ tail â€“f /var/log/apache2/access.log\n  Install Kinesis agent   Then, install and configure the Kinesis Agent\n$ cd ~ $ sudo apt-get install openjdk-8-jdk $ git clone https://github.com/awslabs/amazon-kinesis-agent $ cd amazon-kinesis-agent $ sudo ./setup --install   After the agent is installed, the configuration file can be found in /etc/aws-kinesis/agent.json. Edit the configuration file to send your Apache access log to the web-access-log stream. (Let\u0026rsquo;s not worry about the error log in this tutorial.)\n{ \u0026#34;cloudwatch.emitMetrics\u0026#34;: true, \u0026#34;kinesis.endpoint\u0026#34;: \u0026#34;kinesis.ap-northeast-2.amazonaws.com\u0026#34;, \u0026#34;firehose.endpoint\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;flows\u0026#34;: [ { \u0026#34;filePattern\u0026#34;: \u0026#34;/var/log/apache2/access.log\u0026#34;, \u0026#34;kinesisStream\u0026#34;: \u0026#34;web-access-log\u0026#34;, \u0026#34;partitionKeyOption\u0026#34;: \u0026#34;RANDOM\u0026#34; } ] }   Once you updated the configuration file, you can start the Kinesis Agent using the following command:\n$ sudo service aws-kinesis-agent stop\n$ sudo service aws-kinesis-agent start\n  Then you can check the status of the Kinesis Agent using the following command:\n$ sudo service aws-kinesis-agent status\n  If the agent is not working as expected, look into the logs (under /var/log/aws-kinesis-agent) to understand what is going on. (If there is no log, what would you do?) It is likely that the user running the Kinesis Agent (aws-kinesis-agent-user) does not have access to the Apache logs (/var/log/apache2/).\n  To resolve this issue, you can add the aws-kinesis-agent-user to the adm group.\n$ sudo usermod -a -G adm aws-kinesis-agent-user $ sudo service aws-kinesis-agent stop $ sudo service aws-kinesis-agent start   Refresh your web application in the browser, then watch the Kinesis Agent logs to see whether your logs are pushed to the Kinesis streams. When the Kinesis Agent says the logs are successfully sent to destinations, check the \u0026ldquo;Monitoring\u0026rdquo; tab in the Kinesis data streams console to confirm this.\n  Create a new AMI from the above-mentioned EC2 instance, then create a new launch configuration from the new AMI. Modify your Auto Scaling group to use the new launch configuration. This way, all of the EC2 instance in your web server fleet is capable of sending logs to your Kinesis stream.\n  Now go to the Kinesis Analytics console to create a Kinesis Analytics Application, with the web-access-log data stream as the source. Click on the \u0026ldquo;Discover scheme\u0026rdquo; to automatically discover the scheme in the data, then save the scheme and continue. In the SQL Editor, copy and paste the following sample SQL statements. Then click on the \u0026ldquo;Save and run SQL\u0026rdquo; button to start your application.\n-- Create a destination stream CREATE OR REPLACE STREAM \u0026#34;DESTINATION_SQL_STREAM\u0026#34; (client_ip VARCHAR(16), request_count INTEGER); -- Create a pump which continuously selects from a source stream (SOURCE_SQL_STREAM_001) CREATE OR REPLACE PUMP \u0026#34;STREAM_PUMP\u0026#34; AS INSERT INTO \u0026#34;DESTINATION_SQL_STREAM\u0026#34; -- Aggregation functions COUNT|AVG|MAX|MIN|SUM|STDDEV_POP|STDDEV_SAMP|VAR_POP|VAR_SAMP SELECT STREAM \u0026#34;client_ip\u0026#34;, COUNT(*) AS request_count FROM \u0026#34;SOURCE_SQL_STREAM_001\u0026#34; -- Uses a 10-second tumbling time window GROUP BY \u0026#34;client_ip\u0026#34;, FLOOR((\u0026#34;SOURCE_SQL_STREAM_001\u0026#34;.ROWTIME - TIMESTAMP \u0026#39;1970-01-01 00:00:00\u0026#39;) SECOND / 10 TO SECOND);   From multiple EC2 instances, use the Apache Benchmark tool (ab) to generate some more web traffic. Observe and explain the query results in the Kinesis Analytics console.\n$ ab -n 100000 -c 2 http://\u0026lt;dns-endpoint-of-your-load-balancer\u0026gt;/web-demo/index.php   Secreenshot on Kinesis Analytics "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]